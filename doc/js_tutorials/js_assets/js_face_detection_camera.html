<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Face Detection Camera And Video Example</title>
<link href="js_example_style.css" rel="stylesheet" type="text/css" />
</head>
<body>
<h2>Face Detection Camera And Video Example</h2>
<p>
    Click <b>Start/Stop</b> button to start or stop the capture.<br>
    The <b>videoInput</b> is a &lt;video&gt; element used as face detector input.
    The <b>canvasOutput</b> is a &lt;canvas&gt; element used as face detector output.<br>
    The code of &lt;textarea&gt; will be executed when video is started.
    You can modify the code to investigate more.
</p>
<div>
<div class="control">
    <button id="cameraStartAndStop" disabled>Start Camera</button>
    <button id="videoStartAndStop" disabled>Start Video</button>
</div>
<textarea class="code" rows="29" cols="80" id="codeEditor" spellcheck="false">
</textarea>
</div>
<p class="err" id="errorMessage"></p>
<div>
    <table cellpadding="0" cellspacing="0" width="0" border="0">
    <tr>
        <td>
            <video id="videoInput" width=320 height=240></video>
        </td>
        <td>
            <canvas id="canvasOutput" width=320 height=240></canvas>
        </td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>
            <div class="caption">videoInput</div>
        </td>
        <td>
            <div class="caption">canvasOutput</div>
        </td>
        <td></td>
        <td></td>
    </tr>
    </table>
</div>
<script src="https://webrtc.github.io/adapter/adapter-5.0.4.js" type="text/javascript"></script>
<script src="utils.js" type="text/javascript"></script>
<script id="codeSnippet" type="text/code-snippet">
let video = document.getElementById('videoInput');
let src = new cv.Mat(video.height, video.width, cv.CV_8UC4);
let dst = new cv.Mat(video.height, video.width, cv.CV_8UC4);
let gray = new cv.Mat();
let cap = new cv.VideoCapture(video);
let faces = new cv.RectVector();
let classifier = new cv.CascadeClassifier();

// load pre-trained classifiers
classifier.load('haarcascade_frontalface_default.xml');

const FPS = 30;
function processVideo() {
    try {
        if (!cameraStreaming && !videoStreaming) {
            // clean and stop.
            src.delete();
            dst.delete();
            gray.delete();
            faces.delete();
            classifier.delete();
            return;
        }
        let begin = Date.now();
        // start processing.
        cap.read(src);
        src.copyTo(dst);
        cv.cvtColor(dst, gray, cv.COLOR_RGBA2GRAY, 0);
        // detect faces.
        classifier.detectMultiScale(gray, faces, 1.1, 3, 0);
        // draw faces.
        for (let i = 0; i < faces.size(); ++i) {
            let face = faces.get(i);
            let point1 = new cv.Point(face.x, face.y);
            let point2 = new cv.Point(face.x + face.width, face.y + face.height);
            cv.rectangle(dst, point1, point2, [255, 0, 0, 255]);
        }
        cv.imshow('canvasOutput', dst);
        // schedule the next one.
        let delay = 1000/FPS - (Date.now() - begin);
        setTimeout(processVideo, delay);
    } catch (err) {
        utils.printError(err);
    }
};

// schedule the first one.
setTimeout(processVideo, 0);
</script>
<script type="text/javascript">
let utils = new Utils('errorMessage');

utils.loadCode('codeSnippet', 'codeEditor');

let cameraStreaming = false;
let videoStreaming = false;
let videoInput = document.getElementById('videoInput');
let cameraStartAndStop = document.getElementById('cameraStartAndStop');
let videoStartAndStop = document.getElementById('videoStartAndStop');
let canvasOutput = document.getElementById('canvasOutput');
let canvasContext = canvasOutput.getContext('2d');

cameraStartAndStop.addEventListener('click', () => {
    if (!cameraStreaming) {
        onVideoStopped();
        utils.clearError();
        onCameraStopped();
        utils.startCamera('qvga', onCameraStarted, 'videoInput');
    } else {
        onVideoStopped();
        utils.stopCamera();
        onCameraStopped();
    }
});

function onCameraStarted() {
    cameraStreaming = true;
    cameraStartAndStop.innerText = 'Stop Camera';
    videoInput.width = videoInput.videoWidth;
    videoInput.height = videoInput.videoHeight;
    utils.executeCode('codeEditor');
}

function onCameraStopped() {
    cameraStreaming = false;
    canvasContext.clearRect(0, 0, canvasOutput.width, canvasOutput.height);
    cameraStartAndStop.innerText = 'Start Camera';
}

videoStartAndStop.addEventListener('click', () => {
    if (!videoStreaming) {
        utils.stopCamera();
        onCameraStopped();
        utils.clearError();
        videoInput.play().then(() => {
            onVideoStarted();
        });
    } else {
        utils.stopCamera();
        onCameraStopped();
        videoInput.pause();
        videoInput.currentTime = 0;
        onVideoStopped();
    }
});

function onVideoStarted() {
    videoStreaming = true;
    videoStartAndStop.innerText = 'Stop Video';
    videoInput.height = videoInput.width * (videoInput.videoHeight / videoInput.videoWidth);
    utils.executeCode('codeEditor');
}

function onVideoStopped() {
    videoStreaming = false;
    canvasContext.clearRect(0, 0, canvasOutput.width, canvasOutput.height);
    videoStartAndStop.innerText = 'Start Video';
}

utils.loadOpenCv(() => {
    let faceCascadeFile = 'haarcascade_frontalface_default.xml';
    utils.createFileFromUrl(faceCascadeFile, faceCascadeFile, () => {
        cameraStartAndStop.removeAttribute('disabled');
    });
    videoInput.addEventListener('canplay', () => {
        videoStartAndStop.removeAttribute('disabled');
    });
    videoInput.src = 'Megamind.mp4';
});
</script>
</body>
</html>
